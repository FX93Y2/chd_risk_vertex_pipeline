{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install kfp\n",
        "! pip install google-cloud-pipeline-components\n",
        "! pip install gcsfs\n",
        "! pip install fsspec"
      ],
      "metadata": {
        "id": "3Ei10h1GspDl"
      },
      "id": "3Ei10h1GspDl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "DKblUHrliqJIMUMKgtLZNDQc",
      "metadata": {
        "tags": [],
        "id": "DKblUHrliqJIMUMKgtLZNDQc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714767012251,
          "user_tz": 420,
          "elapsed": 314,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "source": [
        "# Set parameters\n",
        "project_id = 'flash-parity-420600'\n",
        "location = 'us-central1'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kfp\n",
        "from google.cloud import aiplatform\n",
        "aiplatform.init(project=project_id, location=location)\n",
        "from kfp.v2.dsl import pipeline\n",
        "from kfp.v2.dsl import component\n",
        "from kfp.v2.dsl import OutputPath, InputPath, Dataset, Model, Metrics, Input, Output, Artifact, ClassificationMetrics"
      ],
      "metadata": {
        "id": "HMdUnfJEsNrJ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714767018553,
          "user_tz": 420,
          "elapsed": 5533,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dcadbbd-e5df-4607-f6b6-3f9e6be7dcc3"
      },
      "id": "HMdUnfJEsNrJ",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d1295fc3623c>:4: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
            "  from kfp.v2.dsl import pipeline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drop A1C, PaiteintID"
      ],
      "metadata": {
        "id": "ZkJc9LtgCGSC"
      },
      "id": "ZkJc9LtgCGSC"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"gcsfs\", \"fsspec\"])\n",
        "def drop_columns_component(\n",
        "    input_dataset_path: str,\n",
        "    output_dataset_path: OutputPath('Dataset'),\n",
        "    columns_to_drop: list\n",
        "):\n",
        "    import pandas as pd\n",
        "\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(input_dataset_path)\n",
        "\n",
        "    # Drop the specified columns\n",
        "    df = df.drop(columns=columns_to_drop)\n",
        "\n",
        "    # Save the dataset\n",
        "    df.to_csv(output_dataset_path, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhj5LE9gC8kv",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714768580270,
          "user_tz": 420,
          "elapsed": 275,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "4afeccfc-5409-4e04-ad51-76e24963a5d7"
      },
      "id": "hhj5LE9gC8kv",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
            "  return component_factory.create_component_from_func(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data"
      ],
      "metadata": {
        "id": "1Knjo5v5GmtN"
      },
      "id": "1Knjo5v5GmtN"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"gcsfs\", \"fsspec\"])\n",
        "def split_component(\n",
        "    input_dataset_path: InputPath('Dataset'),\n",
        "    train_dataset_path: OutputPath('Dataset'),\n",
        "    validation_dataset_path: OutputPath('Dataset'),\n",
        "    test_size: float = 0.1,\n",
        "    random_state: int = 42):\n",
        "    import pandas as pd\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(input_dataset_path)\n",
        "\n",
        "    # Split into train+validation and test\n",
        "    train_df, val_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    # Save the split datasets\n",
        "    train_df.to_csv(train_dataset_path, index=False)\n",
        "    val_df.to_csv(validation_dataset_path, index=False)\n"
      ],
      "metadata": {
        "id": "H59NHRLQVRG6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714768582184,
          "user_tz": 420,
          "elapsed": 289,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "H59NHRLQVRG6",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Impute train"
      ],
      "metadata": {
        "id": "s3Nq_N4MOLrK"
      },
      "id": "s3Nq_N4MOLrK"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"numpy\", \"gcsfs\", \"fsspec\"])\n",
        "def train_impute_data_component(\n",
        "    train_dataset_path: InputPath('Dataset'),\n",
        "    imputed_train_dataset_path: OutputPath('Dataset'),\n",
        "    imputation_params_path: OutputPath(),\n",
        "    imputation_metadata: Output[Artifact]  # Output for metadata\n",
        "):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import json\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    train_df = pd.read_csv(train_dataset_path)\n",
        "    # Handling 'cigsPerDay' custom imputation\n",
        "    train_df.loc[train_df['currentSmoker'] == 0, 'cigsPerDay'] = train_df.loc[train_df['currentSmoker'] == 0, 'cigsPerDay'].fillna(0)\n",
        "    median_cigs = train_df[train_df['currentSmoker'] == 1]['cigsPerDay'].median()\n",
        "    train_df.loc[train_df['currentSmoker'] == 1, 'cigsPerDay'] = train_df.loc[train_df['currentSmoker'] == 1, 'cigsPerDay'].fillna(median_cigs)\n",
        "    # General median and mode imputation\n",
        "    median_imputer = SimpleImputer(strategy='median')\n",
        "    mode_imputer = SimpleImputer(strategy='most_frequent')\n",
        "    median_cols = ['glucose', 'totChol', 'BMI', 'heartRate']\n",
        "    mode_cols = ['education', 'BPMeds']\n",
        "    train_df[median_cols] = median_imputer.fit_transform(train_df[median_cols])\n",
        "    train_df[mode_cols] = mode_imputer.fit_transform(train_df[mode_cols])\n",
        "\n",
        "    # Convert 'BPMeds' and 'Education' to integer\n",
        "    train_df['BPMeds'] = train_df['BPMeds'].astype(int)\n",
        "    train_df['education'] = train_df['education'].astype(int)\n",
        "\n",
        "    imputation_params = {\n",
        "        'median_cigs': median_cigs,\n",
        "        'median_values': median_imputer.statistics_.tolist(),\n",
        "        'mode_values': mode_imputer.statistics_.tolist()\n",
        "    }\n",
        "    with open(imputation_params_path, 'w') as f:\n",
        "        json.dump(imputation_params, f)\n",
        "\n",
        "    # Saving metadata\n",
        "    imputation_metadata.metadata = imputation_params\n",
        "    train_df.to_csv(imputed_train_dataset_path, index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "saN-I4tWzb1O",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714768584003,
          "user_tz": 420,
          "elapsed": 241,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "saN-I4tWzb1O",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Impute Validation"
      ],
      "metadata": {
        "id": "2QJDP2pI7lQg"
      },
      "id": "2QJDP2pI7lQg"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"numpy\", \"gcsfs\", \"fsspec\"])\n",
        "def validation_impute_data_component(\n",
        "    validation_dataset_path: InputPath('Dataset'),\n",
        "    imputation_params_path: InputPath(),\n",
        "    imputed_validation_dataset_path: OutputPath('Dataset'),\n",
        "    validation_imputation_metadata: Output[Artifact]  # Output for metadata\n",
        "):\n",
        "    import pandas as pd\n",
        "    import json\n",
        "    import numpy as np\n",
        "    from sklearn.impute import SimpleImputer\n",
        "\n",
        "    # Load the validation dataset\n",
        "    validation_df = pd.read_csv(validation_dataset_path)\n",
        "\n",
        "    # Load imputation parameters from the JSON file\n",
        "    with open(imputation_params_path, 'r') as f:\n",
        "        imputation_params = json.load(f)\n",
        "\n",
        "    # Applying custom imputation for 'cigsPerDay'\n",
        "    validation_df.loc[validation_df['currentSmoker'] == 0, 'cigsPerDay'] = validation_df.loc[validation_df['currentSmoker'] == 0, 'cigsPerDay'].fillna(0)\n",
        "    validation_df.loc[validation_df['currentSmoker'] == 1, 'cigsPerDay'] = validation_df.loc[validation_df['currentSmoker'] == 1, 'cigsPerDay'].fillna(imputation_params['median_cigs'])\n",
        "\n",
        "    # Applying general median and mode imputation using parameters from the training dataset\n",
        "    median_cols = ['glucose', 'totChol', 'BMI', 'heartRate']\n",
        "    mode_cols = ['education', 'BPMeds']\n",
        "    median_imputer = SimpleImputer(strategy='median')\n",
        "    mode_imputer = SimpleImputer(strategy='most_frequent')\n",
        "    median_imputer.statistics_ = np.array(imputation_params['median_values'])\n",
        "    mode_imputer.statistics_ = np.array(imputation_params['mode_values'])\n",
        "    validation_df[median_cols] = median_imputer.transform(validation_df[median_cols])\n",
        "    validation_df[mode_cols] = mode_imputer.transform(validation_df[mode_cols])\n",
        "\n",
        "    # Convert 'BPMeds' and 'Education' to integer\n",
        "    validation_df['BPMeds'] = validation_df['BPMeds'].astype(int)\n",
        "    validation_df['education'] = validation_df['education'].astype(int)\n",
        "\n",
        "    # Save the imputed validation dataset\n",
        "    validation_df.to_csv(imputed_validation_dataset_path, index=False)\n",
        "\n",
        "    # Saving metadata for imputation\n",
        "    validation_imputation_metadata.metadata = {\n",
        "        \"median_cigs_used\": imputation_params['median_cigs'],\n",
        "        \"median_values_used\": imputation_params['median_values'],\n",
        "        \"mode_values_used\": imputation_params['mode_values']\n",
        "    }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0QXYWQ3N7E5y",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714768587280,
          "user_tz": 420,
          "elapsed": 280,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "0QXYWQ3N7E5y",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Impute Evaluation"
      ],
      "metadata": {
        "id": "IKKZg1NuaU0Y"
      },
      "id": "IKKZg1NuaU0Y"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"numpy\", \"gcsfs\", \"fsspec\"])\n",
        "def evaluation_impute_data_component(\n",
        "    evaluation_dataset_path: InputPath('Dataset'),\n",
        "    imputation_params_path: str,\n",
        "    imputed_evaluation_dataset_path: OutputPath('Dataset'),\n",
        "    evaluation_imputation_metadata: Output[Artifact]  # Output for metadata\n",
        "):\n",
        "    import pandas as pd\n",
        "    import json\n",
        "    import numpy as np\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    import gcsfs\n",
        "    # Load the evaluation dataset\n",
        "    evaluation_df = pd.read_csv(evaluation_dataset_path)\n",
        "    # Load imputation parameters from the JSON file\n",
        "    fs = gcsfs.GCSFileSystem()\n",
        "    with fs.open(imputation_params_path, 'r') as f:\n",
        "        imputation_params = json.load(f)\n",
        "    # Applying custom imputation for 'cigsPerDay'\n",
        "    evaluation_df.loc[evaluation_df['currentSmoker'] == 0, 'cigsPerDay'] = evaluation_df.loc[evaluation_df['currentSmoker'] == 0, 'cigsPerDay'].fillna(0)\n",
        "    evaluation_df.loc[evaluation_df['currentSmoker'] == 1, 'cigsPerDay'] = evaluation_df.loc[evaluation_df['currentSmoker'] == 1, 'cigsPerDay'].fillna(imputation_params['median_cigs'])\n",
        "    # Applying general median and mode imputation using parameters from the training dataset\n",
        "    median_cols = ['glucose', 'totChol', 'BMI', 'heartRate']\n",
        "    mode_cols = ['education', 'BPMeds']\n",
        "    median_imputer = SimpleImputer(strategy='median')\n",
        "    mode_imputer = SimpleImputer(strategy='most_frequent')\n",
        "    median_imputer.statistics_ = np.array(imputation_params['median_values'])\n",
        "    mode_imputer.statistics_ = np.array(imputation_params['mode_values'])\n",
        "    evaluation_df[median_cols] = median_imputer.transform(evaluation_df[median_cols])\n",
        "    evaluation_df[mode_cols] = mode_imputer.transform(evaluation_df[mode_cols])\n",
        "    # Convert 'BPMeds' and 'Education' to integer\n",
        "    evaluation_df['BPMeds'] = evaluation_df['BPMeds'].astype(int)\n",
        "    evaluation_df['education'] = evaluation_df['education'].astype(int)\n",
        "    # Save the imputed evaluation dataset\n",
        "    evaluation_df.to_csv(imputed_evaluation_dataset_path, index=False)\n",
        "    # Saving metadata for imputation\n",
        "    evaluation_imputation_metadata.metadata = {\n",
        "        \"median_cigs_used\": imputation_params['median_cigs'],\n",
        "        \"median_values_used\": imputation_params['median_values'],\n",
        "        \"mode_values_used\": imputation_params['mode_values']\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOltTeU7abQX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714797788552,
          "user_tz": 420,
          "elapsed": 449,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "deb84097-a52c-4123-92ca-14bdb7aa356d"
      },
      "id": "UOltTeU7abQX",
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
            "  return component_factory.create_component_from_func(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Winsorize train"
      ],
      "metadata": {
        "id": "MwyJL6tpJGn3"
      },
      "id": "MwyJL6tpJGn3"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"numpy\", \"gcsfs\", \"fsspec\"])\n",
        "def train_winsorize_component(\n",
        "    train_dataset_path: InputPath('Dataset'),\n",
        "    winsorized_train_dataset_path: OutputPath('Dataset'),\n",
        "    winsorize_params_path: OutputPath(),\n",
        "    winsorize_metadata: Output[Artifact]  # Output for metadata\n",
        "):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import json\n",
        "\n",
        "    train_df = pd.read_csv(train_dataset_path)\n",
        "\n",
        "    numerical_columns = ['age', 'cigsPerDay', 'totChol', 'income', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\n",
        "    winsorize_thresholds = {}\n",
        "\n",
        "    for col in numerical_columns:\n",
        "        sorted_train_data = np.sort(train_df[col])\n",
        "        lower_limit_index = int(0.01 * len(sorted_train_data))  # 1% lower limit\n",
        "        upper_limit_index = int(0.99 * len(sorted_train_data)) - 1  # 99% upper limit\n",
        "        winsorize_thresholds[col] = (float(sorted_train_data[lower_limit_index]), float(sorted_train_data[upper_limit_index]))\n",
        "\n",
        "        # Apply winsorization to train dataset\n",
        "        train_df[col] = np.where(train_df[col] < sorted_train_data[lower_limit_index], sorted_train_data[lower_limit_index], train_df[col])\n",
        "        train_df[col] = np.where(train_df[col] > sorted_train_data[upper_limit_index], sorted_train_data[upper_limit_index], train_df[col])\n",
        "\n",
        "    # Save winsorize parameters\n",
        "    with open(winsorize_params_path, 'w') as f:\n",
        "        json.dump(winsorize_thresholds, f)\n",
        "\n",
        "    # Saving metadata\n",
        "    winsorize_metadata.metadata = {col: list(thresholds) for col, thresholds in winsorize_thresholds.items()}\n",
        "\n",
        "    train_df.to_csv(winsorized_train_dataset_path, index=False)\n"
      ],
      "metadata": {
        "id": "mDl9AE0WJJdo",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714769531755,
          "user_tz": 420,
          "elapsed": 257,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "mDl9AE0WJJdo",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Winsorize validation"
      ],
      "metadata": {
        "id": "8QI-I638J1N-"
      },
      "id": "8QI-I638J1N-"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"numpy\", \"gcsfs\", \"fsspec\"])\n",
        "def validation_winsorize_component(\n",
        "    validation_dataset_path: InputPath('Dataset'),\n",
        "    winsorize_params_path: InputPath(),\n",
        "    winsorized_validation_dataset_path: OutputPath('Dataset'),\n",
        "    validation_winsorize_metadata: Output[Artifact]  # Output for metadata\n",
        "):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import json\n",
        "\n",
        "    validation_df = pd.read_csv(validation_dataset_path)\n",
        "\n",
        "    # Load winsorize parameters from the JSON file\n",
        "    with open(winsorize_params_path, 'r') as f:\n",
        "        winsorize_thresholds = json.load(f)\n",
        "\n",
        "    numerical_columns = ['age', 'cigsPerDay', 'totChol', 'income', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\n",
        "\n",
        "    for col in numerical_columns:\n",
        "        lower_limit, upper_limit = winsorize_thresholds[col]\n",
        "\n",
        "        # Apply winsorization to validation dataset\n",
        "        validation_df[col] = np.where(validation_df[col] < lower_limit, lower_limit, validation_df[col])\n",
        "        validation_df[col] = np.where(validation_df[col] > upper_limit, upper_limit, validation_df[col])\n",
        "\n",
        "    # Saving metadata\n",
        "    validation_winsorize_metadata.metadata = {col: list(thresholds) for col, thresholds in winsorize_thresholds.items()}\n",
        "\n",
        "    validation_df.to_csv(winsorized_validation_dataset_path, index=False)"
      ],
      "metadata": {
        "id": "IPDUPN_MJ3Qu",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714769534392,
          "user_tz": 420,
          "elapsed": 352,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "IPDUPN_MJ3Qu",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Winsorize Evaluation"
      ],
      "metadata": {
        "id": "m8x5k00LbzAn"
      },
      "id": "m8x5k00LbzAn"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"numpy\", \"gcsfs\", \"fsspec\"])\n",
        "def evaluation_winsorize_component(\n",
        "    evaluation_dataset_path: InputPath('Dataset'),\n",
        "    winsorize_params_path: str,\n",
        "    winsorized_evaluation_dataset_path: OutputPath('Dataset'),\n",
        "    evaluation_winsorize_metadata: Output[Artifact]  # Output for metadata\n",
        "):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import json\n",
        "    import gcsfs\n",
        "\n",
        "    evaluation_df = pd.read_csv(evaluation_dataset_path)\n",
        "\n",
        "    # Load winsorize parameters from the JSON file\n",
        "    fs = gcsfs.GCSFileSystem()\n",
        "    with fs.open(winsorize_params_path, 'r') as f:\n",
        "        winsorize_thresholds = json.load(f)\n",
        "\n",
        "    numerical_columns = ['age', 'cigsPerDay', 'totChol', 'income', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\n",
        "\n",
        "    for col in numerical_columns:\n",
        "        lower_limit, upper_limit = winsorize_thresholds[col]\n",
        "\n",
        "        # Apply winsorization to evaluation dataset\n",
        "        evaluation_df[col] = np.where(evaluation_df[col] < lower_limit, lower_limit, evaluation_df[col])\n",
        "        evaluation_df[col] = np.where(evaluation_df[col] > upper_limit, upper_limit, evaluation_df[col])\n",
        "\n",
        "    # Saving metadata\n",
        "    evaluation_winsorize_metadata.metadata = {col: list(thresholds) for col, thresholds in winsorize_thresholds.items()}\n",
        "\n",
        "    evaluation_df.to_csv(winsorized_evaluation_dataset_path, index=False)"
      ],
      "metadata": {
        "id": "GFBf5oo6b1w5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714797791830,
          "user_tz": 420,
          "elapsed": 300,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "GFBf5oo6b1w5",
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log Transformation Train"
      ],
      "metadata": {
        "id": "zTq3L_7qMzD7"
      },
      "id": "zTq3L_7qMzD7"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"numpy\", \"gcsfs\", \"fsspec\"])\n",
        "def train_log_transform_component(\n",
        "    train_dataset_path: InputPath('Dataset'),\n",
        "    log_transformed_train_dataset_path: OutputPath('Dataset')\n",
        "):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    train_df = pd.read_csv(train_dataset_path)\n",
        "\n",
        "    skewed_features = ['age', 'BMI', 'heartRate', 'totChol', 'glucose']\n",
        "    for feature in skewed_features:\n",
        "        train_df[feature] = np.log1p(train_df[feature])\n",
        "\n",
        "    train_df.to_csv(log_transformed_train_dataset_path, index=False)\n"
      ],
      "metadata": {
        "id": "9j2-ZOHHMxHu",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714769536177,
          "user_tz": 420,
          "elapsed": 254,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "9j2-ZOHHMxHu",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log Transformation Validation"
      ],
      "metadata": {
        "id": "8cKq-4peN5j8"
      },
      "id": "8cKq-4peN5j8"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"numpy\", \"gcsfs\", \"fsspec\"])\n",
        "def validation_log_transform_component(\n",
        "    validation_dataset_path: InputPath('Dataset'),\n",
        "    log_transformed_validation_dataset_path: OutputPath('Dataset')\n",
        "):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    validation_df = pd.read_csv(validation_dataset_path)\n",
        "\n",
        "    skewed_features = ['age', 'BMI', 'heartRate', 'totChol', 'glucose']\n",
        "    for feature in skewed_features:\n",
        "        validation_df[feature] = np.log1p(validation_df[feature])\n",
        "\n",
        "    validation_df.to_csv(log_transformed_validation_dataset_path, index=False)"
      ],
      "metadata": {
        "id": "vfXSTzDMN_sp",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714769538442,
          "user_tz": 420,
          "elapsed": 290,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "vfXSTzDMN_sp",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log Transformation Evaluation"
      ],
      "metadata": {
        "id": "p9gfMI_PcDVM"
      },
      "id": "p9gfMI_PcDVM"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"numpy\", \"gcsfs\", \"fsspec\"])\n",
        "def evaluation_log_transform_component(\n",
        "    evaluation_dataset_path: InputPath('Dataset'),\n",
        "    log_transformed_evaluation_dataset_path: OutputPath('Dataset')\n",
        "):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    evaluation_df = pd.read_csv(evaluation_dataset_path)\n",
        "\n",
        "    skewed_features = ['age', 'BMI', 'heartRate', 'totChol', 'glucose']\n",
        "    for feature in skewed_features:\n",
        "        evaluation_df[feature] = np.log1p(evaluation_df[feature])\n",
        "\n",
        "    evaluation_df.to_csv(log_transformed_evaluation_dataset_path, index=False)"
      ],
      "metadata": {
        "id": "_Lf0slXzcI94",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714789931158,
          "user_tz": 420,
          "elapsed": 284,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "_Lf0slXzcI94",
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Box-cox Transform Train"
      ],
      "metadata": {
        "id": "YAo8LrhDOGI4"
      },
      "id": "YAo8LrhDOGI4"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"numpy\", \"scikit-learn\", \"scipy\", \"gcsfs\", \"fsspec\"])\n",
        "def train_boxcox_transform_component(\n",
        "    train_dataset_path: InputPath('Dataset'),\n",
        "    boxcox_transformed_train_dataset_path: OutputPath('Dataset'),\n",
        "    boxcox_transformers_path: OutputPath(),\n",
        "    boxcox_metadata: Output[Artifact]\n",
        "):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from scipy.stats import boxcox\n",
        "    from sklearn.preprocessing import PowerTransformer\n",
        "    import pickle\n",
        "\n",
        "    train_df = pd.read_csv(train_dataset_path)\n",
        "\n",
        "    skewed_features_second = ['BMI', 'heartRate', 'glucose']\n",
        "    transformers = {feature: PowerTransformer(method='box-cox', standardize=False) for feature in skewed_features_second}\n",
        "    for feature in skewed_features_second:\n",
        "        train_df[feature] = transformers[feature].fit_transform(train_df[[feature]].values)\n",
        "\n",
        "    with open(boxcox_transformers_path, 'wb') as f:\n",
        "        pickle.dump(transformers, f)\n",
        "\n",
        "    boxcox_metadata.metadata = {feature: float(transformer.lambdas_[0]) for feature, transformer in transformers.items()}\n",
        "\n",
        "    train_df.to_csv(boxcox_transformed_train_dataset_path, index=False)"
      ],
      "metadata": {
        "id": "e0DKT0JSOKyS",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714770293598,
          "user_tz": 420,
          "elapsed": 358,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "e0DKT0JSOKyS",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Box-cox Transform Validation"
      ],
      "metadata": {
        "id": "OrRzEFv8OO7Y"
      },
      "id": "OrRzEFv8OO7Y"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"numpy\", \"scikit-learn\", \"scipy\", \"gcsfs\", \"fsspec\"])\n",
        "def validation_boxcox_transform_component(\n",
        "    validation_dataset_path: InputPath('Dataset'),\n",
        "    boxcox_transformers_path: InputPath(),\n",
        "    boxcox_transformed_validation_dataset_path: OutputPath('Dataset')\n",
        "):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from scipy.stats import boxcox\n",
        "    from sklearn.preprocessing import PowerTransformer\n",
        "    import pickle\n",
        "\n",
        "    validation_df = pd.read_csv(validation_dataset_path)\n",
        "\n",
        "    with open(boxcox_transformers_path, 'rb') as f:\n",
        "        transformers = pickle.load(f)\n",
        "\n",
        "    skewed_features_second = ['BMI', 'heartRate', 'glucose']\n",
        "    for feature in skewed_features_second:\n",
        "        validation_df[feature] = transformers[feature].transform(validation_df[[feature]].values)\n",
        "\n",
        "    validation_df.to_csv(boxcox_transformed_validation_dataset_path, index=False)"
      ],
      "metadata": {
        "id": "h-i8Wq3IOU3o",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714770297812,
          "user_tz": 420,
          "elapsed": 425,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "h-i8Wq3IOU3o",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Box-cox Transform Evaluation"
      ],
      "metadata": {
        "id": "xjmpYCJKcM22"
      },
      "id": "xjmpYCJKcM22"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"numpy\", \"scikit-learn\", \"scipy\", \"gcsfs\", \"fsspec\"])\n",
        "def evaluation_boxcox_transform_component(\n",
        "    evaluation_dataset_path: InputPath('Dataset'),\n",
        "    boxcox_transformers_path: str,\n",
        "    boxcox_transformed_evaluation_dataset_path: OutputPath('Dataset')\n",
        "):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from scipy.stats import boxcox\n",
        "    from sklearn.preprocessing import PowerTransformer\n",
        "    import pickle\n",
        "    import gcsfs\n",
        "\n",
        "    evaluation_df = pd.read_csv(evaluation_dataset_path)\n",
        "\n",
        "    fs = gcsfs.GCSFileSystem()\n",
        "    with fs.open(boxcox_transformers_path, 'rb') as f:\n",
        "        transformers = pickle.load(f)\n",
        "\n",
        "    skewed_features_second = ['BMI', 'heartRate', 'glucose']\n",
        "    for feature in skewed_features_second:\n",
        "        evaluation_df[feature] = transformers[feature].transform(evaluation_df[[feature]].values)\n",
        "\n",
        "    evaluation_df.to_csv(boxcox_transformed_evaluation_dataset_path, index=False)"
      ],
      "metadata": {
        "id": "8MfCl-5ccRni",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714797799439,
          "user_tz": 420,
          "elapsed": 436,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "8MfCl-5ccRni",
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Binning train"
      ],
      "metadata": {
        "id": "5FlpaTjFUeGz"
      },
      "id": "5FlpaTjFUeGz"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"numpy\", \"gcsfs\", \"fsspec\"])\n",
        "def train_binning_component(\n",
        "    train_dataset_path: InputPath('Dataset'),\n",
        "    binned_train_dataset_path: OutputPath('Dataset'),\n",
        "    bin_edges_path: OutputPath(),\n",
        "    binning_metadata: Output[Artifact]\n",
        "):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import json\n",
        "\n",
        "    train_df = pd.read_csv(train_dataset_path)\n",
        "\n",
        "    q = train_df['income'].quantile([0.25, 0.5, 0.75]).values\n",
        "    train_df['income_category'] = pd.qcut(train_df['income'], q=[0, 0.25, 0.5, 0.75, 1], labels=['Low', 'Middle', 'High', 'Very High'])\n",
        "    train_df.drop(columns='income', inplace=True)\n",
        "\n",
        "    with open(bin_edges_path, 'w') as f:\n",
        "        json.dump({'q': q.tolist()}, f)\n",
        "\n",
        "    binning_metadata.metadata = {'bin_edges': q.tolist()}\n",
        "\n",
        "    train_df.to_csv(binned_train_dataset_path, index=False)"
      ],
      "metadata": {
        "id": "KfcNRadAUhc1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714771350313,
          "user_tz": 420,
          "elapsed": 496,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "KfcNRadAUhc1",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binning validation"
      ],
      "metadata": {
        "id": "dcSo9zQnUxzQ"
      },
      "id": "dcSo9zQnUxzQ"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"numpy\", \"gcsfs\", \"fsspec\"])\n",
        "def validation_binning_component(\n",
        "    validation_dataset_path: InputPath('Dataset'),\n",
        "    bin_edges_path: InputPath(),\n",
        "    binned_validation_dataset_path: OutputPath('Dataset')\n",
        "):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import json\n",
        "\n",
        "    validation_df = pd.read_csv(validation_dataset_path)\n",
        "\n",
        "    with open(bin_edges_path, 'r') as f:\n",
        "        bin_edges = json.load(f)['q']\n",
        "\n",
        "    validation_df['income_category'] = pd.cut(validation_df['income'], bins=[validation_df['income'].min()] + bin_edges + [validation_df['income'].max()], labels=['Low', 'Middle', 'High', 'Very High'])\n",
        "    validation_df.drop(columns='income', inplace=True)\n",
        "\n",
        "    validation_df.to_csv(binned_validation_dataset_path, index=False)\n"
      ],
      "metadata": {
        "id": "oW52UrwKUzxl",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714771352610,
          "user_tz": 420,
          "elapsed": 298,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "oW52UrwKUzxl",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binning evaluation"
      ],
      "metadata": {
        "id": "XobHgz_ucW06"
      },
      "id": "XobHgz_ucW06"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"numpy\", \"gcsfs\", \"fsspec\"])\n",
        "def evaluation_binning_component(\n",
        "    evaluation_dataset_path: InputPath('Dataset'),\n",
        "    bin_edges_path: str,\n",
        "    binned_evaluation_dataset_path: OutputPath('Dataset')\n",
        "):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import json\n",
        "    import gcsfs\n",
        "\n",
        "    fs = gcsfs.GCSFileSystem()\n",
        "\n",
        "    evaluation_df = pd.read_csv(evaluation_dataset_path)\n",
        "\n",
        "    with fs.open(bin_edges_path, 'r') as f:\n",
        "        bin_edges = json.load(f)['q']\n",
        "\n",
        "    evaluation_df['income_category'] = pd.cut(evaluation_df['income'], bins=[evaluation_df['income'].min()] + bin_edges + [evaluation_df['income'].max()], labels=['Low', 'Middle', 'High', 'Very High'])\n",
        "    evaluation_df.drop(columns='income', inplace=True)\n",
        "\n",
        "    evaluation_df.to_csv(binned_evaluation_dataset_path, index=False)"
      ],
      "metadata": {
        "id": "T0aqx9V9cZtS",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714797802546,
          "user_tz": 420,
          "elapsed": 2,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "T0aqx9V9cZtS",
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Combine Train"
      ],
      "metadata": {
        "id": "Pta_FyDwU7g4"
      },
      "id": "Pta_FyDwU7g4"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"gcsfs\", \"fsspec\"])\n",
        "def train_feature_combine_component(\n",
        "    train_dataset_path: InputPath('Dataset'),\n",
        "    combined_train_dataset_path: OutputPath('Dataset')\n",
        "):\n",
        "    import pandas as pd\n",
        "\n",
        "    train_df = pd.read_csv(train_dataset_path)\n",
        "\n",
        "    def categorize_smoking(row):\n",
        "        if row['currentSmoker'] == 0:\n",
        "            return 'non-smoker'\n",
        "        elif row['cigsPerDay'] <= 5:\n",
        "            return 'light smoker'\n",
        "        elif row['cigsPerDay'] <= 20:\n",
        "            return 'moderate smoker'\n",
        "        else:\n",
        "            return 'heavy smoker'\n",
        "\n",
        "    train_df['smoking_status'] = train_df.apply(categorize_smoking, axis=1)\n",
        "    train_df.drop(columns=['currentSmoker', 'cigsPerDay'], inplace=True)\n",
        "\n",
        "    def create_bp_category(row):\n",
        "        if row['prevalentHyp'] == 1:\n",
        "            return 'Hypertension'\n",
        "        else:\n",
        "            if row['sysBP'] < 120 and row['diaBP'] < 80:\n",
        "                return 'Normal'\n",
        "            elif (row['sysBP'] >= 120 and row['sysBP'] < 140) or (row['diaBP'] >= 80 and row['diaBP'] < 90):\n",
        "                return 'Prehypertension'\n",
        "            elif row['sysBP'] >= 140 or row['diaBP'] >= 90:\n",
        "                return 'Hypertension'\n",
        "\n",
        "    train_df['bp_category'] = train_df.apply(create_bp_category, axis=1)\n",
        "    train_df = train_df.drop(['sysBP', 'diaBP', 'prevalentHyp'], axis=1)\n",
        "\n",
        "    train_df.to_csv(combined_train_dataset_path, index=False)"
      ],
      "metadata": {
        "id": "2qADu6gKVFZN",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714771346530,
          "user_tz": 420,
          "elapsed": 358,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "2qADu6gKVFZN",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Combine Validation"
      ],
      "metadata": {
        "id": "9okVoXfKVQiP"
      },
      "id": "9okVoXfKVQiP"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"gcsfs\", \"fsspec\"])\n",
        "def validation_feature_combine_component(\n",
        "    validation_dataset_path: InputPath('Dataset'),\n",
        "    combined_validation_dataset_path: OutputPath('Dataset')\n",
        "):\n",
        "    import pandas as pd\n",
        "\n",
        "    validation_df = pd.read_csv(validation_dataset_path)\n",
        "\n",
        "    def categorize_smoking(row):\n",
        "        if row['currentSmoker'] == 0:\n",
        "            return 'non-smoker'\n",
        "        elif row['cigsPerDay'] <= 5:\n",
        "            return 'light smoker'\n",
        "        elif row['cigsPerDay'] <= 20:\n",
        "            return 'moderate smoker'\n",
        "        else:\n",
        "            return 'heavy smoker'\n",
        "\n",
        "    validation_df['smoking_status'] = validation_df.apply(categorize_smoking, axis=1)\n",
        "    validation_df.drop(columns=['currentSmoker', 'cigsPerDay'], inplace=True)\n",
        "\n",
        "    def create_bp_category(row):\n",
        "        if row['prevalentHyp'] == 1:\n",
        "            return 'Hypertension'\n",
        "        else:\n",
        "            if row['sysBP'] < 120 and row['diaBP'] < 80:\n",
        "                return 'Normal'\n",
        "            elif (row['sysBP'] >= 120 and row['sysBP'] < 140) or (row['diaBP'] >= 80 and row['diaBP'] < 90):\n",
        "                return 'Prehypertension'\n",
        "            elif row['sysBP'] >= 140 or row['diaBP'] >= 90:\n",
        "                return 'Hypertension'\n",
        "\n",
        "    validation_df['bp_category'] = validation_df.apply(create_bp_category, axis=1)\n",
        "    validation_df = validation_df.drop(['sysBP', 'diaBP', 'prevalentHyp'], axis=1)\n",
        "\n",
        "    validation_df.to_csv(combined_validation_dataset_path, index=False)"
      ],
      "metadata": {
        "id": "7tw6Bx0pVWSf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714771357298,
          "user_tz": 420,
          "elapsed": 435,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "7tw6Bx0pVWSf",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Combine Evaluation"
      ],
      "metadata": {
        "id": "QXKPqSX5cg4b"
      },
      "id": "QXKPqSX5cg4b"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"gcsfs\", \"fsspec\"])\n",
        "def evaluation_feature_combine_component(\n",
        "    evaluation_dataset_path: InputPath('Dataset'),\n",
        "    combined_evaluation_dataset_path: OutputPath('Dataset')\n",
        "):\n",
        "    import pandas as pd\n",
        "\n",
        "    evaluation_df = pd.read_csv(evaluation_dataset_path)\n",
        "\n",
        "    def categorize_smoking(row):\n",
        "        if row['currentSmoker'] == 0:\n",
        "            return 'non-smoker'\n",
        "        elif row['cigsPerDay'] <= 5:\n",
        "            return 'light smoker'\n",
        "        elif row['cigsPerDay'] <= 20:\n",
        "            return 'moderate smoker'\n",
        "        else:\n",
        "            return 'heavy smoker'\n",
        "\n",
        "    evaluation_df['smoking_status'] = evaluation_df.apply(categorize_smoking, axis=1)\n",
        "    evaluation_df.drop(columns=['currentSmoker', 'cigsPerDay'], inplace=True)\n",
        "\n",
        "    def create_bp_category(row):\n",
        "        if row['prevalentHyp'] == 1:\n",
        "            return 'Hypertension'\n",
        "        else:\n",
        "            if row['sysBP'] < 120 and row['diaBP'] < 80:\n",
        "                return 'Normal'\n",
        "            elif (row['sysBP'] >= 120 and row['sysBP'] < 140) or (row['diaBP'] >= 80 and row['diaBP'] < 90):\n",
        "                return 'Prehypertension'\n",
        "            elif row['sysBP'] >= 140 or row['diaBP'] >= 90:\n",
        "                return 'Hypertension'\n",
        "\n",
        "    evaluation_df['bp_category'] = evaluation_df.apply(create_bp_category, axis=1)\n",
        "    evaluation_df = evaluation_df.drop(['sysBP', 'diaBP', 'prevalentHyp'], axis=1)\n",
        "\n",
        "    evaluation_df.to_csv(combined_evaluation_dataset_path, index=False)"
      ],
      "metadata": {
        "id": "snqgZDBqclKZ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714789978339,
          "user_tz": 420,
          "elapsed": 300,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "snqgZDBqclKZ",
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode train"
      ],
      "metadata": {
        "id": "wB6g4BJ9b6Fd"
      },
      "id": "wB6g4BJ9b6Fd"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"gcsfs\", \"fsspec\"])\n",
        "def train_encoding_component(\n",
        "    train_dataset_path: InputPath('Dataset'),\n",
        "    encoded_train_dataset_path: OutputPath('Dataset')\n",
        "):\n",
        "    import pandas as pd\n",
        "\n",
        "    train_df = pd.read_csv(train_dataset_path)\n",
        "    train_df = pd.get_dummies(train_df, columns=['income_category', 'smoking_status', 'bp_category'])\n",
        "\n",
        "    train_df.to_csv(encoded_train_dataset_path, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR3r4mqFf4LI",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714774100480,
          "user_tz": 420,
          "elapsed": 287,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "e409f932-ad9e-4ef6-b46d-45bda0bcce36"
      },
      "id": "rR3r4mqFf4LI",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
            "  return component_factory.create_component_from_func(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encode validation"
      ],
      "metadata": {
        "id": "r4TwOpgPf6UY"
      },
      "id": "r4TwOpgPf6UY"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"gcsfs\", \"fsspec\"])\n",
        "def validation_encoding_component(\n",
        "    validation_dataset_path: InputPath('Dataset'),\n",
        "    encoded_validation_dataset_path: OutputPath('Dataset')\n",
        "):\n",
        "    import pandas as pd\n",
        "\n",
        "    validation_df = pd.read_csv(validation_dataset_path)\n",
        "    validation_df = pd.get_dummies(validation_df, columns=['income_category', 'smoking_status', 'bp_category'])\n",
        "\n",
        "    validation_df.to_csv(encoded_validation_dataset_path, index=False)\n"
      ],
      "metadata": {
        "id": "ONmyiQiWf8WU",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714774107011,
          "user_tz": 420,
          "elapsed": 287,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "ONmyiQiWf8WU",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encode evaluation"
      ],
      "metadata": {
        "id": "yhTxbEOBcr0d"
      },
      "id": "yhTxbEOBcr0d"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"gcsfs\", \"fsspec\"])\n",
        "def evaluation_encoding_component(\n",
        "    evaluation_dataset_path: InputPath('Dataset'),\n",
        "    encoded_evaluation_dataset_path: OutputPath('Dataset')\n",
        "):\n",
        "    import pandas as pd\n",
        "\n",
        "    evaluation_df = pd.read_csv(evaluation_dataset_path)\n",
        "    evaluation_df = pd.get_dummies(evaluation_df, columns=['income_category', 'smoking_status', 'bp_category'])\n",
        "\n",
        "    evaluation_df.to_csv(encoded_evaluation_dataset_path, index=False)"
      ],
      "metadata": {
        "id": "10vENhBscvUu",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714790074526,
          "user_tz": 420,
          "elapsed": 252,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "10vENhBscvUu",
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling train"
      ],
      "metadata": {
        "id": "2GDxnA5Pj6sN"
      },
      "id": "2GDxnA5Pj6sN"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"gcsfs\", \"fsspec\"])\n",
        "def train_scaling_component(\n",
        "    train_dataset_path: InputPath('Dataset'),\n",
        "    scaled_train_features_path: OutputPath('Dataset'),\n",
        "    scaled_train_target_path: OutputPath('Dataset'),\n",
        "    scaler_path: OutputPath(),\n",
        "    scaling_metadata: Output[Artifact]\n",
        "):\n",
        "    import pandas as pd\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import pickle\n",
        "\n",
        "    train_df = pd.read_csv(train_dataset_path)\n",
        "\n",
        "    X_train = train_df.drop('TenYearCHD', axis=1)\n",
        "    y_train = train_df['TenYearCHD']\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    numerical_features = ['age', 'totChol', 'BMI', 'heartRate', 'glucose']\n",
        "    X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
        "\n",
        "    with open(scaler_path, 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "\n",
        "    scaling_metadata.metadata = {\n",
        "        'mean': scaler.mean_.tolist(),\n",
        "        'scale': scaler.scale_.tolist()\n",
        "    }\n",
        "\n",
        "    X_train.to_csv(scaled_train_features_path, index=False)\n",
        "    y_train.to_csv(scaled_train_target_path, index=False)"
      ],
      "metadata": {
        "id": "xK3aIrhrl8Y1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714784791277,
          "user_tz": 420,
          "elapsed": 448,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "xK3aIrhrl8Y1",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scaling validation"
      ],
      "metadata": {
        "id": "hgOAgpHDl9fK"
      },
      "id": "hgOAgpHDl9fK"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"gcsfs\", \"fsspec\"])\n",
        "def validation_scaling_component(\n",
        "    validation_dataset_path: InputPath('Dataset'),\n",
        "    scaler_path: InputPath(),\n",
        "    scaled_validation_features_path: OutputPath('Dataset'),\n",
        "    scaled_validation_target_path: OutputPath('Dataset')\n",
        "):\n",
        "    import pandas as pd\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import pickle\n",
        "\n",
        "    validation_df = pd.read_csv(validation_dataset_path)\n",
        "\n",
        "    X_val = validation_df.drop('TenYearCHD', axis=1)\n",
        "    y_val = validation_df['TenYearCHD']\n",
        "\n",
        "    with open(scaler_path, 'rb') as f:\n",
        "        scaler = pickle.load(f)\n",
        "\n",
        "    numerical_features = ['age', 'totChol', 'BMI', 'heartRate', 'glucose']\n",
        "    X_val[numerical_features] = scaler.transform(X_val[numerical_features])\n",
        "\n",
        "    X_val.to_csv(scaled_validation_features_path, index=False)\n",
        "    y_val.to_csv(scaled_validation_target_path, index=False)"
      ],
      "metadata": {
        "id": "1z70RQpimA9D",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714784789243,
          "user_tz": 420,
          "elapsed": 386,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "1z70RQpimA9D",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling Evaluation"
      ],
      "metadata": {
        "id": "bDAQqtiPc3va"
      },
      "id": "bDAQqtiPc3va"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"gcsfs\", \"fsspec\"])\n",
        "def evaluation_scaling_component(\n",
        "    evaluation_dataset_path: InputPath('Dataset'),\n",
        "    scaler_path: str,\n",
        "    scaled_evaluation_features_path: OutputPath('Dataset')\n",
        "):\n",
        "    import pandas as pd\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import pickle\n",
        "    import gcsfs\n",
        "\n",
        "    evaluation_df = pd.read_csv(evaluation_dataset_path)\n",
        "    fs = gcsfs.GCSFileSystem()\n",
        "    with fs.open(scaler_path, 'rb') as f:\n",
        "        scaler = pickle.load(f)\n",
        "\n",
        "    numerical_features = ['age', 'totChol', 'BMI', 'heartRate', 'glucose']\n",
        "    evaluation_df[numerical_features] = scaler.transform(evaluation_df[numerical_features])\n",
        "\n",
        "    evaluation_df.to_csv(scaled_evaluation_features_path, index=False)"
      ],
      "metadata": {
        "id": "yTR6Mv5Sc3Ru",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714797810739,
          "user_tz": 420,
          "elapsed": 432,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "yTR6Mv5Sc3Ru",
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Voting Ensemble"
      ],
      "metadata": {
        "id": "0Z2LX97PMFKb"
      },
      "id": "0Z2LX97PMFKb"
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2.dsl import Metrics\n",
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"lightgbm\", \"imbalanced-learn==0.9.0\", \"gcsfs\", \"fsspec\"])\n",
        "def voting_ensemble_component(\n",
        "    train_features_path: InputPath('Dataset'),\n",
        "    train_target_path: InputPath('Dataset'),\n",
        "    validation_features_path: InputPath('Dataset'),\n",
        "    validation_target_path: InputPath('Dataset'),\n",
        "    model_path: OutputPath('Model'),\n",
        "    evaluation_metrics: Output[Metrics]):\n",
        "    import pandas as pd\n",
        "    from sklearn.ensemble import VotingClassifier\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.svm import SVC\n",
        "    from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "    import lightgbm as lgb\n",
        "    from sklearn.naive_bayes import GaussianNB\n",
        "    from sklearn.metrics import accuracy_score, f1_score\n",
        "    import pickle\n",
        "    X_train = pd.read_csv(train_features_path)\n",
        "    y_train = pd.read_csv(train_target_path)\n",
        "    X_val = pd.read_csv(validation_features_path)\n",
        "    y_val = pd.read_csv(validation_target_path)\n",
        "    # Create instances of individual models\n",
        "    lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "    svm = SVC(kernel='sigmoid', class_weight='balanced', probability=True, random_state=42)\n",
        "    brf = BalancedRandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "    lgbm = lgb.LGBMClassifier(num_leaves=63, max_depth=-1, learning_rate=0.05, n_estimators=1000, scale_pos_weight=1, random_state=42)\n",
        "    gn = GaussianNB()\n",
        "    estimators = [\n",
        "        ('lr', lr),\n",
        "        ('svm', svm),\n",
        "        ('brf', brf),\n",
        "        ('lgbm', lgbm),\n",
        "        ('gn', gn)\n",
        "    ]\n",
        "    model = VotingClassifier(estimators=estimators, voting='soft')\n",
        "    model.fit(X_train, y_train)\n",
        "    # Save the trained model\n",
        "    with open(model_path, 'wb') as file:\n",
        "        pickle.dump(model, file)\n",
        "    # Predict and evaluate\n",
        "    y_pred = model.predict(X_val)\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
        "    evaluation_metrics.log_metric('accuracy', accuracy)\n",
        "    evaluation_metrics.log_metric('f1_score', f1)\n"
      ],
      "metadata": {
        "id": "sgwgOIQsMJt6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714786534930,
          "user_tz": 420,
          "elapsed": 311,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "sgwgOIQsMJt6",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prediction"
      ],
      "metadata": {
        "id": "X3D5USPSRJQj"
      },
      "id": "X3D5USPSRJQj"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"lightgbm\", \"imbalanced-learn==0.9.0\", \"gcsfs\", \"fsspec\"])\n",
        "def prediction_component(\n",
        "    evaluation_dataset_path: InputPath('Dataset'),\n",
        "    original_evaluation_dataset_path: str,\n",
        "    model_path: str,\n",
        "    predictions_path: OutputPath('Dataset')\n",
        "):\n",
        "    import pandas as pd\n",
        "    import pickle\n",
        "    import gcsfs\n",
        "\n",
        "    # Load the preprocessed evaluation dataset\n",
        "    evaluation_data = pd.read_csv(evaluation_dataset_path)\n",
        "\n",
        "    # Load the original evaluation dataset\n",
        "    original_evaluation_data = pd.read_csv(original_evaluation_dataset_path)\n",
        "\n",
        "    # Load the trained model\n",
        "    fs = gcsfs.GCSFileSystem()\n",
        "    with fs.open(model_path, 'rb') as file:\n",
        "        model = pickle.load(file)\n",
        "\n",
        "    # Make predictions on the evaluation dataset\n",
        "    predictions = model.predict(evaluation_data)\n",
        "\n",
        "    # Create a DataFrame with the patientID and predictions\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'patientID': original_evaluation_data['patientID'],\n",
        "        'TenYearCHD': predictions\n",
        "    })\n",
        "\n",
        "    # Save the predictions with patientID to a CSV file\n",
        "    predictions_df.to_csv(predictions_path, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pslhAi2nRIUb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714798894997,
          "user_tz": 420,
          "elapsed": 307,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "c5f0082c-5dd1-4899-c831-649c8cbb96cc"
      },
      "id": "pslhAi2nRIUb",
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
            "  return component_factory.create_component_from_func(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Initial Pipeline"
      ],
      "metadata": {
        "id": "Vz8N9C-FN1pw"
      },
      "id": "Vz8N9C-FN1pw"
    },
    {
      "cell_type": "code",
      "source": [
        "@pipeline(name=\"chd-prediction-pipeline\")\n",
        "def chd_prediction_pipeline(input_dataset_path: str):\n",
        "    # Run the components in order\n",
        "    processed_dataset_path = drop_columns_component(input_dataset_path=input_dataset_path,\n",
        "                                                    columns_to_drop=['patientID', 'a1c'])\n",
        "    split_result = split_component(input_dataset_path=processed_dataset_path.output)\n",
        "\n",
        "    train_impute_result = train_impute_data_component(\n",
        "        train_dataset_path=split_result.outputs['train_dataset_path']\n",
        "    )\n",
        "\n",
        "    validation_impute_result = validation_impute_data_component(\n",
        "        validation_dataset_path=split_result.outputs['validation_dataset_path'],\n",
        "        imputation_params_path=train_impute_result.outputs['imputation_params_path']\n",
        "    )\n",
        "\n",
        "    train_winsorize_result = train_winsorize_component(\n",
        "        train_dataset_path=train_impute_result.outputs['imputed_train_dataset_path']\n",
        "    )\n",
        "\n",
        "    validation_winsorize_result = validation_winsorize_component(\n",
        "        validation_dataset_path=validation_impute_result.outputs['imputed_validation_dataset_path'],\n",
        "        winsorize_params_path=train_winsorize_result.outputs['winsorize_params_path']\n",
        "    )\n",
        "\n",
        "    train_log_transform_result = train_log_transform_component(\n",
        "        train_dataset_path=train_winsorize_result.outputs['winsorized_train_dataset_path']\n",
        "    )\n",
        "\n",
        "    validation_log_transform_result = validation_log_transform_component(\n",
        "        validation_dataset_path=validation_winsorize_result.outputs['winsorized_validation_dataset_path']\n",
        "    )\n",
        "\n",
        "    train_boxcox_transform_result = train_boxcox_transform_component(\n",
        "        train_dataset_path=train_log_transform_result.outputs['log_transformed_train_dataset_path']\n",
        "    )\n",
        "\n",
        "    validation_boxcox_transform_result = validation_boxcox_transform_component(\n",
        "        validation_dataset_path=validation_log_transform_result.outputs['log_transformed_validation_dataset_path'],\n",
        "        boxcox_transformers_path=train_boxcox_transform_result.outputs['boxcox_transformers_path']\n",
        "    )\n",
        "\n",
        "    train_binning_result = train_binning_component(\n",
        "        train_dataset_path=train_boxcox_transform_result.outputs['boxcox_transformed_train_dataset_path']\n",
        "    )\n",
        "\n",
        "    validation_binning_result = validation_binning_component(\n",
        "        validation_dataset_path=validation_boxcox_transform_result.outputs['boxcox_transformed_validation_dataset_path'],\n",
        "        bin_edges_path=train_binning_result.outputs['bin_edges_path']\n",
        "    )\n",
        "\n",
        "    train_feature_combine_result = train_feature_combine_component(\n",
        "        train_dataset_path=train_binning_result.outputs['binned_train_dataset_path']\n",
        "    )\n",
        "\n",
        "    validation_feature_combine_result = validation_feature_combine_component(\n",
        "        validation_dataset_path=validation_binning_result.outputs['binned_validation_dataset_path']\n",
        "    )\n",
        "\n",
        "    train_encoding_result = train_encoding_component(\n",
        "        train_dataset_path=train_feature_combine_result.outputs['combined_train_dataset_path']\n",
        "    )\n",
        "\n",
        "    validation_encoding_result = validation_encoding_component(\n",
        "        validation_dataset_path=validation_feature_combine_result.outputs['combined_validation_dataset_path']\n",
        "    )\n",
        "\n",
        "    train_scaling_result = train_scaling_component(\n",
        "        train_dataset_path=train_encoding_result.outputs['encoded_train_dataset_path']\n",
        "    )\n",
        "\n",
        "    validation_scaling_result = validation_scaling_component(\n",
        "        validation_dataset_path=validation_encoding_result.outputs['encoded_validation_dataset_path'],\n",
        "        scaler_path=train_scaling_result.outputs['scaler_path']\n",
        "    )\n",
        "\n",
        "    voting_ensemble_result = voting_ensemble_component(\n",
        "        train_features_path=train_scaling_result.outputs['scaled_train_features_path'],\n",
        "        train_target_path=train_scaling_result.outputs['scaled_train_target_path'],\n",
        "        validation_features_path=validation_scaling_result.outputs['scaled_validation_features_path'],\n",
        "        validation_target_path=validation_scaling_result.outputs['scaled_validation_target_path']\n",
        "    )\n"
      ],
      "metadata": {
        "id": "7cOTAQX-N4Zh",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714794538756,
          "user_tz": 420,
          "elapsed": 223,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "7cOTAQX-N4Zh",
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile and run Initial Pipeline"
      ],
      "metadata": {
        "id": "LWYLrd7ZXm39"
      },
      "id": "LWYLrd7ZXm39"
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2 import compiler\n",
        "\n",
        "compiler.Compiler().compile(\n",
        "    pipeline_func=chd_prediction_pipeline,\n",
        "    package_path = 'chd_prediction_pipeline.json'\n",
        ")\n",
        "\n",
        "pipeline_job = aiplatform.PipelineJob(\n",
        "    display_name='chd-prediction-pipeline',\n",
        "    template_path='chd_prediction_pipeline.json',\n",
        "    pipeline_root='gs://ise543-final-project-yfang',\n",
        "    enable_caching=True,\n",
        "    parameter_values={\n",
        "        'input_dataset_path': 'gs://ise543-final-project-yfang/Final Project Dataset.csv',\n",
        "    }\n",
        ")\n",
        "\n",
        "pipeline_job.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlQ7RfvIXpS8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714794559287,
          "user_tz": 420,
          "elapsed": 17401,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "4b8b0cc3-f14b-43ef-c572-69e161cc07d7"
      },
      "id": "GlQ7RfvIXpS8",
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/990976262210/locations/us-central1/pipelineJobs/chd-prediction-pipeline-20240504034902\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/990976262210/locations/us-central1/pipelineJobs/chd-prediction-pipeline-20240504034902')\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/chd-prediction-pipeline-20240504034902?project=990976262210\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/990976262210/locations/us-central1/pipelineJobs/chd-prediction-pipeline-20240504034902 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/990976262210/locations/us-central1/pipelineJobs/chd-prediction-pipeline-20240504034902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference Pipeline"
      ],
      "metadata": {
        "id": "kCHxNrlHsHzJ"
      },
      "id": "kCHxNrlHsHzJ"
    },
    {
      "cell_type": "code",
      "source": [
        "imputation_params_path = 'gs://ise543-final-project-yfang/990976262210/chd-prediction-pipeline-20240503212758/train-impute-data-component_7933296970762813440/imputation_params_path'\n",
        "winsorize_params_path = 'gs://ise543-final-project-yfang/990976262210/chd-prediction-pipeline-20240503212758/train-winsorize-component_2168689447728578560/winsorize_params_path'\n",
        "boxcox_transformers_path = 'gs://ise543-final-project-yfang/990976262210/chd-prediction-pipeline-20240503212758/train-boxcox-transform-component_3321610952335425536/boxcox_transformers_path'\n",
        "bin_edges_path = 'gs://ise543-final-project-yfang/990976262210/chd-prediction-pipeline-20240503212758/train-binning-component_-3595918075305656320/bin_edges_path'\n",
        "scaler_path = 'gs://ise543-final-project-yfang/990976262210/chd-prediction-pipeline-20240504010640/train-scaling-component_471676813140033536/scaler_path'\n",
        "model_path = 'gs://ise543-final-project-yfang/990976262210/chd-prediction-pipeline-20240504013540/voting-ensemble-component_-2551364436732411904/model_path'\n",
        "\n",
        "@pipeline(name=\"chd-prediction-inference-pipeline\")\n",
        "def chd_prediction_inference_pipeline(\n",
        "    evaluation_dataset_path: str,\n",
        "    model_path: str = model_path,\n",
        "    imputation_params_path: str = imputation_params_path,\n",
        "    winsorize_params_path: str = winsorize_params_path,\n",
        "    boxcox_transformers_path: str = boxcox_transformers_path,\n",
        "    bin_edges_path: str = bin_edges_path,\n",
        "    scaler_path: str = scaler_path\n",
        "):\n",
        "    # Preprocessing components for the evaluation dataset\n",
        "    processed_evaluation_dataset_path = drop_columns_component(input_dataset_path=evaluation_dataset_path, columns_to_drop=['patientID', 'a1c'])\n",
        "\n",
        "    evaluation_impute_result = evaluation_impute_data_component(\n",
        "        evaluation_dataset_path=processed_evaluation_dataset_path.output,\n",
        "        imputation_params_path=imputation_params_path\n",
        "    )\n",
        "\n",
        "    evaluation_winsorize_result = evaluation_winsorize_component(\n",
        "        evaluation_dataset_path=evaluation_impute_result.outputs['imputed_evaluation_dataset_path'],\n",
        "        winsorize_params_path=winsorize_params_path\n",
        "    )\n",
        "\n",
        "    evaluation_log_transform_result = evaluation_log_transform_component(\n",
        "        evaluation_dataset_path=evaluation_winsorize_result.outputs['winsorized_evaluation_dataset_path']\n",
        "    )\n",
        "\n",
        "    evaluation_boxcox_transform_result = evaluation_boxcox_transform_component(\n",
        "        evaluation_dataset_path=evaluation_log_transform_result.outputs['log_transformed_evaluation_dataset_path'],\n",
        "        boxcox_transformers_path=boxcox_transformers_path\n",
        "    )\n",
        "\n",
        "    evaluation_binning_result = evaluation_binning_component(\n",
        "        evaluation_dataset_path=evaluation_boxcox_transform_result.outputs['boxcox_transformed_evaluation_dataset_path'],\n",
        "        bin_edges_path=bin_edges_path\n",
        "    )\n",
        "\n",
        "    evaluation_feature_combine_result = evaluation_feature_combine_component(\n",
        "        evaluation_dataset_path=evaluation_binning_result.outputs['binned_evaluation_dataset_path']\n",
        "    )\n",
        "\n",
        "    evaluation_encoding_result = evaluation_encoding_component(\n",
        "        evaluation_dataset_path=evaluation_feature_combine_result.outputs['combined_evaluation_dataset_path']\n",
        "    )\n",
        "\n",
        "    evaluation_scaling_result = evaluation_scaling_component(\n",
        "        evaluation_dataset_path=evaluation_encoding_result.outputs['encoded_evaluation_dataset_path'],\n",
        "        scaler_path=scaler_path\n",
        "    )\n",
        "\n",
        "    # Prediction component\n",
        "    prediction_result = prediction_component(\n",
        "        evaluation_dataset_path=evaluation_scaling_result.outputs['scaled_evaluation_features_path'],\n",
        "        original_evaluation_dataset_path=evaluation_dataset_path,\n",
        "        model_path=model_path\n",
        "    )"
      ],
      "metadata": {
        "id": "b9UtgvXWyYKO",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714798904333,
          "user_tz": 420,
          "elapsed": 392,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "b9UtgvXWyYKO",
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile and run Inference Pipeline"
      ],
      "metadata": {
        "id": "8Y08apmBuSfB"
      },
      "id": "8Y08apmBuSfB"
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2 import compiler\n",
        "\n",
        "compiler.Compiler().compile(\n",
        "    pipeline_func=chd_prediction_inference_pipeline,\n",
        "    package_path='chd_prediction_inference_pipeline.json'\n",
        ")\n",
        "\n",
        "pipeline_job = aiplatform.PipelineJob(\n",
        "    display_name='chd-prediction-inference-pipeline',\n",
        "    template_path='chd_prediction_inference_pipeline.json',\n",
        "    pipeline_root='gs://ise543-final-project-yfang',\n",
        "    enable_caching=True,\n",
        "    parameter_values={\n",
        "        'evaluation_dataset_path': 'gs://ise543-final-project-yfang/Final-Project-Evaluation-Dataset.csv'\n",
        "    }\n",
        ")\n",
        "\n",
        "pipeline_job.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JB9PUr9twAc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714799003638,
          "user_tz": 420,
          "elapsed": 96184,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "6784cf47-c0c0-4f5c-b3f2-f47e59129723"
      },
      "id": "4JB9PUr9twAc",
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/990976262210/locations/us-central1/pipelineJobs/chd-prediction-inference-pipeline-20240504050147\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/990976262210/locations/us-central1/pipelineJobs/chd-prediction-inference-pipeline-20240504050147')\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/chd-prediction-inference-pipeline-20240504050147?project=990976262210\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/990976262210/locations/us-central1/pipelineJobs/chd-prediction-inference-pipeline-20240504050147 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/990976262210/locations/us-central1/pipelineJobs/chd-prediction-inference-pipeline-20240504050147 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/990976262210/locations/us-central1/pipelineJobs/chd-prediction-inference-pipeline-20240504050147 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/990976262210/locations/us-central1/pipelineJobs/chd-prediction-inference-pipeline-20240504050147 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/990976262210/locations/us-central1/pipelineJobs/chd-prediction-inference-pipeline-20240504050147\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "yanf1202 (May 2, 2024, 4:40:51 PM)",
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}